{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Sankytanky100/Data-Engineering/blob/main/Pyspark_in_Colab_Tutorial_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "This tutorial introduces PySpark fundamentals in Google Colab, focusing on environment setup, DataFrame creation, and core transformations/actions.\n",
    "\n",
    "**You will learn**\n",
    "- How to initialize a Spark session in Colab.\n",
    "- Common DataFrame operations (select, filter, withColumn, aggregations).\n",
    "- How to inspect and validate results at each step.\n",
    "\n",
    "**Prerequisites**\n",
    "- Colab runtime with internet access.\n",
    "- PySpark installation (handled in the setup cells).\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Install Java\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "# Install PySpark\n",
    "!pip install pyspark\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mQzTdDNZ_4Nb",
    "outputId": "dd4a88ea-d7cf-40ef-8008-a7ad3b1a88e8"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Check Java version\n",
    "!java -version\n",
    "\n",
    "# Check PySpark version\n",
    "!pyspark --version\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rVBm6awO_6o4",
    "outputId": "7c4a3187-4c7c-4944-ed4a-23dcf3b33b5a"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "openjdk version \"11.0.25\" 2024-10-15\n",
      "OpenJDK Runtime Environment (build 11.0.25+9-post-Ubuntu-1ubuntu122.04)\n",
      "OpenJDK 64-Bit Server VM (build 11.0.25+9-post-Ubuntu-1ubuntu122.04, mixed mode, sharing)\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.3\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 11.0.25\n",
      "Branch HEAD\n",
      "Compiled by user haejoon.lee on 2024-09-09T05:20:05Z\n",
      "Revision 32232e9ed33bb16b93ad58cfde8b82e0f07c0970\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark on Google Colab\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify SparkSession\n",
    "print(\"SparkSession created:\", spark)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LmXq0Fdc_79d",
    "outputId": "03e9f053-2781-479b-969c-e8a32238ff74"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SparkSession created: <pyspark.sql.session.SparkSession object at 0x7912c237a470>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "student_data = [(\"Chris\",1523,0.72,\"CA\"),\n",
    "                (\"Jake\", 1555,0.83,\"NY\"),\n",
    "                (\"Cody\", 1439,0.92,\"CA\"),\n",
    "                (\"Lisa\",1442,0.81,\"FL\"),\n",
    "                (\"Daniel\",1600,0.88,\"TX\"),\n",
    "                (\"Kelvin\",1382,0.99,\"FL\"),\n",
    "                (\"Nancy\",1442,0.74,\"TX\"),\n",
    "                (\"Pavel\",1599,0.82,\"NY\"),\n",
    "                (\"Josh\",1482,0.78,\"CA\"),\n",
    "                (\"Cynthia\",1582,0.94,\"CA\")]\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "student_rdd = spark.sparkContext.parallelize(student_data, 5)"
   ],
   "metadata": {
    "id": "CyemSsQFAyHj"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# confirm your RDD contains the correct data\n",
    "student_rdd.collect()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pTpsP-yCA4N7",
    "outputId": "d55728e6-d82c-4f4b-cfe7-3a317177f00c"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Chris', 1523, 0.72, 'CA'),\n",
       " ('Jake', 1555, 0.83, 'NY'),\n",
       " ('Cody', 1439, 0.92, 'CA'),\n",
       " ('Lisa', 1442, 0.81, 'FL'),\n",
       " ('Daniel', 1600, 0.88, 'TX'),\n",
       " ('Kelvin', 1382, 0.99, 'FL'),\n",
       " ('Nancy', 1442, 0.74, 'TX'),\n",
       " ('Pavel', 1599, 0.82, 'NY'),\n",
       " ('Josh', 1482, 0.78, 'CA'),\n",
       " ('Cynthia', 1582, 0.94, 'CA')]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "student_rdd.getNumPartitions()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q7xSqTAwA7jG",
    "outputId": "e9ec413b-d565-4b48-afef-5f91afb7604f"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "rdd_transformation = student_rdd.map(lambda x: (x[0], x[1], int(x[2]*100), x[3]))\n",
    "\n",
    "# confirm transformation is correct\n",
    "rdd_transformation.collect()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IoUOjOVtBncy",
    "outputId": "283c2de1-6af0-42bf-db41-4ca7afb244f8"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Chris', 1523, 72, 'CA'),\n",
       " ('Jake', 1555, 83, 'NY'),\n",
       " ('Cody', 1439, 92, 'CA'),\n",
       " ('Lisa', 1442, 81, 'FL'),\n",
       " ('Daniel', 1600, 88, 'TX'),\n",
       " ('Kelvin', 1382, 99, 'FL'),\n",
       " ('Nancy', 1442, 74, 'TX'),\n",
       " ('Pavel', 1599, 82, 'NY'),\n",
       " ('Josh', 1482, 78, 'CA'),\n",
       " ('Cynthia', 1582, 94, 'CA')]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "sum_gpa = rdd_transformation.map(lambda x: x[2]).reduce(lambda x,y: x+y)\n",
    "\n",
    "# view the sum\n",
    "sum_gpa"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aO91xllWCLAJ",
    "outputId": "51e2767f-80a2-49d2-a432-f8a5f88da4ba"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "843"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "sum_gpa / rdd_transformation.count()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7GMhpw_DCOjB",
    "outputId": "729fac88-30f7-45ff-de9f-7027a8eff77b"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "84.3"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Associative and Commutative Properties**"
   ],
   "metadata": {
    "id": "Y4nSIj9FC1mO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ],
   "metadata": {
    "id": "xCq0wE99C7Xq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data = [1,2,3,4,5]\n",
    "for i in range(1,5):\n",
    "    rdd = spark.sparkContext.parallelize(data, i)\n",
    "    print('partition: ', rdd.glom().collect())\n",
    "    print('addition: ', rdd.reduce(lambda a,b: a+b))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3REu9LigDBgS",
    "outputId": "bf55f383-7bf8-4fe3-c1dc-1932f7fd345d"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "partition:  [[1, 2, 3, 4, 5]]\n",
      "addition:  15\n",
      "partition:  [[1, 2], [3, 4, 5]]\n",
      "addition:  15\n",
      "partition:  [[1], [2, 3], [4, 5]]\n",
      "addition:  15\n",
      "partition:  [[1], [2], [3], [4, 5]]\n",
      "addition:  15\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(1,5):\n",
    "    rdd = spark.sparkContext.parallelize(data, i)\n",
    "    print('partition: ', rdd.glom().collect())\n",
    "    print('division: ', rdd.reduce(lambda a,b: a/b))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CqCIy8jdDIBQ",
    "outputId": "fe687010-544b-4684-e704-5400ae55c799"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "partition:  [[1, 2, 3, 4, 5]]\n",
      "division:  0.008333333333333333\n",
      "partition:  [[1, 2], [3, 4, 5]]\n",
      "division:  3.3333333333333335\n",
      "partition:  [[1], [2, 3], [4, 5]]\n",
      "division:  1.875\n",
      "partition:  [[1], [2], [3], [4, 5]]\n",
      "division:  0.20833333333333331\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Accumulator & Broadcast Variables**"
   ],
   "metadata": {
    "id": "TCRHUNOFD9hv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "student_data = [(\"Chris\",1523,0.72,\"CA\"),\n",
    "                (\"Jake\", 1555,0.83,\"NY\"),\n",
    "                (\"Cody\", 1439,0.92,\"CA\"),\n",
    "                (\"Lisa\",1442,0.81,\"FL\"),\n",
    "                (\"Daniel\",1600,0.88,\"TX\"),\n",
    "                (\"Kelvin\",1382,0.99,\"FL\"),\n",
    "                (\"Nancy\",1442,0.74,\"TX\"),\n",
    "                (\"Pavel\",1599,0.82,\"NY\"),\n",
    "                (\"Josh\",1482,0.78,\"CA\"),\n",
    "                (\"Cynthia\",1582,0.94,\"CA\")]\n",
    "student_rdd = spark.sparkContext.parallelize(student_data)\n",
    "rdd_transformation = student_rdd.map(lambda x: (x[0], x[1], int(x[2]*100), x[3]))\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"TX\":\"Texas\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "rdd_broadcast = rdd_transformation.map(lambda x: (x[0],x[1],x[2],broadcastStates.value[x[3]]))"
   ],
   "metadata": {
    "id": "yA2vYliUE1O5"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sat_1500 = spark.sparkContext.accumulator(0)\n",
    "\n",
    "# confirm type\n",
    "type(sat_1500)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "3sCMOPfGE7qd",
    "outputId": "2bae41c2-07d3-4528-e4a7-1f736e331aa1"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pyspark.accumulators.Accumulator"
      ],
      "text/html": [
       "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
       "      pre.function-repr-contents {\n",
       "        overflow-x: auto;\n",
       "        padding: 8px 12px;\n",
       "        max-height: 500px;\n",
       "      }\n",
       "\n",
       "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
       "        cursor: pointer;\n",
       "        max-height: 100px;\n",
       "      }\n",
       "    </style>\n",
       "    <pre style=\"white-space: initial; background:\n",
       "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
       "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.accumulators.Accumulator</b><br/>def __init__(aid: int, value: T, accum_param: &#x27;AccumulatorParam[T]&#x27;)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pyspark/accumulators.py</a>A shared variable that can be accumulated, i.e., has a commutative and associative &quot;add&quot;\n",
       "operation. Worker tasks on a Spark cluster can add values to an Accumulator with the `+=`\n",
       "operator, but only the driver program is allowed to access its value, using `value`.\n",
       "Updates from the workers get propagated automatically to the driver program.\n",
       "\n",
       "While :class:`SparkContext` supports accumulators for primitive data types like :class:`int` and\n",
       ":class:`float`, users can also define accumulators for custom types by providing a custom\n",
       ":py:class:`AccumulatorParam` object. Refer to its doctest for an example.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "&gt;&gt;&gt; a = sc.accumulator(1)\n",
       "&gt;&gt;&gt; a.value\n",
       "1\n",
       "&gt;&gt;&gt; a.value = 2\n",
       "&gt;&gt;&gt; a.value\n",
       "2\n",
       "&gt;&gt;&gt; a += 5\n",
       "&gt;&gt;&gt; a.value\n",
       "7\n",
       "&gt;&gt;&gt; sc.accumulator(1.0).value\n",
       "1.0\n",
       "&gt;&gt;&gt; sc.accumulator(1j).value\n",
       "1j\n",
       "&gt;&gt;&gt; rdd = sc.parallelize([1,2,3])\n",
       "&gt;&gt;&gt; def f(x):\n",
       "...     global a\n",
       "...     a += x\n",
       "...\n",
       "&gt;&gt;&gt; rdd.foreach(f)\n",
       "&gt;&gt;&gt; a.value\n",
       "13\n",
       "&gt;&gt;&gt; b = sc.accumulator(0)\n",
       "&gt;&gt;&gt; def g(x):\n",
       "...     b.add(x)\n",
       "...\n",
       "&gt;&gt;&gt; rdd.foreach(g)\n",
       "&gt;&gt;&gt; b.value\n",
       "6\n",
       "\n",
       "&gt;&gt;&gt; rdd.map(lambda x: a.value).collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
       "Traceback (most recent call last):\n",
       "    ...\n",
       "Py4JJavaError: ...\n",
       "\n",
       "&gt;&gt;&gt; def h(x):\n",
       "...     global a\n",
       "...     a.value = 7\n",
       "...\n",
       "&gt;&gt;&gt; rdd.foreach(h) # doctest: +IGNORE_EXCEPTION_DETAIL\n",
       "Traceback (most recent call last):\n",
       "    ...\n",
       "Py4JJavaError: ...\n",
       "\n",
       "&gt;&gt;&gt; sc.accumulator([1.0, 2.0, 3.0]) # doctest: +IGNORE_EXCEPTION_DETAIL\n",
       "Traceback (most recent call last):\n",
       "    ...\n",
       "TypeError: ...</pre>\n",
       "      <script>\n",
       "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
       "        for (const element of document.querySelectorAll('.filepath')) {\n",
       "          element.style.display = 'block'\n",
       "          element.onclick = (event) => {\n",
       "            event.preventDefault();\n",
       "            event.stopPropagation();\n",
       "            google.colab.files.view(element.textContent, 60);\n",
       "          };\n",
       "        }\n",
       "      }\n",
       "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
       "        element.onclick = (event) => {\n",
       "          event.preventDefault();\n",
       "          event.stopPropagation();\n",
       "          element.classList.toggle('function-repr-contents-collapsed');\n",
       "        };\n",
       "      }\n",
       "      </script>\n",
       "      </div>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def count_high_sat_score(r):\n",
    "    if r[1] > 1500: sat_1500.add(1)\n",
    "\n",
    "# confirm saved as a function\n",
    "print(count_high_sat_score)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRVPXFuqE8n4",
    "outputId": "ccaba59b-4c99-480a-c4a1-66667520ac57"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<function count_high_sat_score at 0x7912c2442320>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "rdd_broadcast.foreach(lambda x: count_high_sat_score(x))\n",
    "\n",
    "# confirm accumulator worked\n",
    "print(sat_1500)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ApDkdLScFAL9",
    "outputId": "8a4629d0-c83f-4e94-faf0-58e357bd63c7"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "spark.stop()\n"
   ],
   "metadata": {
    "id": "mik-XWtgAC0z"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}