{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sankytanky100/Data-Engineering/blob/main/Pyspark_in_Colab_Tutorial_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install PySpark\n",
        "!pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQzTdDNZ_4Nb",
        "outputId": "dd4a88ea-d7cf-40ef-8008-a7ad3b1a88e8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Java version\n",
        "!java -version\n",
        "\n",
        "# Check PySpark version\n",
        "!pyspark --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVBm6awO_6o4",
        "outputId": "7c4a3187-4c7c-4944-ed4a-23dcf3b33b5a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.25\" 2024-10-15\n",
            "OpenJDK Runtime Environment (build 11.0.25+9-post-Ubuntu-1ubuntu122.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.25+9-post-Ubuntu-1ubuntu122.04, mixed mode, sharing)\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.3\n",
            "      /_/\n",
            "                        \n",
            "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 11.0.25\n",
            "Branch HEAD\n",
            "Compiled by user haejoon.lee on 2024-09-09T05:20:05Z\n",
            "Revision 32232e9ed33bb16b93ad58cfde8b82e0f07c0970\n",
            "Url https://github.com/apache/spark\n",
            "Type --help for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PySpark on Google Colab\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Verify SparkSession\n",
        "print(\"SparkSession created:\", spark)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmXq0Fdc_79d",
        "outputId": "03e9f053-2781-479b-969c-e8a32238ff74"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkSession created: <pyspark.sql.session.SparkSession object at 0x7912c237a470>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student_data = [(\"Chris\",1523,0.72,\"CA\"),\n",
        "                (\"Jake\", 1555,0.83,\"NY\"),\n",
        "                (\"Cody\", 1439,0.92,\"CA\"),\n",
        "                (\"Lisa\",1442,0.81,\"FL\"),\n",
        "                (\"Daniel\",1600,0.88,\"TX\"),\n",
        "                (\"Kelvin\",1382,0.99,\"FL\"),\n",
        "                (\"Nancy\",1442,0.74,\"TX\"),\n",
        "                (\"Pavel\",1599,0.82,\"NY\"),\n",
        "                (\"Josh\",1482,0.78,\"CA\"),\n",
        "                (\"Cynthia\",1582,0.94,\"CA\")]\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "student_rdd = spark.sparkContext.parallelize(student_data, 5)"
      ],
      "metadata": {
        "id": "CyemSsQFAyHj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm your RDD contains the correct data\n",
        "student_rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTpsP-yCA4N7",
        "outputId": "d55728e6-d82c-4f4b-cfe7-3a317177f00c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Chris', 1523, 0.72, 'CA'),\n",
              " ('Jake', 1555, 0.83, 'NY'),\n",
              " ('Cody', 1439, 0.92, 'CA'),\n",
              " ('Lisa', 1442, 0.81, 'FL'),\n",
              " ('Daniel', 1600, 0.88, 'TX'),\n",
              " ('Kelvin', 1382, 0.99, 'FL'),\n",
              " ('Nancy', 1442, 0.74, 'TX'),\n",
              " ('Pavel', 1599, 0.82, 'NY'),\n",
              " ('Josh', 1482, 0.78, 'CA'),\n",
              " ('Cynthia', 1582, 0.94, 'CA')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student_rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7xSqTAwA7jG",
        "outputId": "e9ec413b-d565-4b48-afef-5f91afb7604f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_transformation = student_rdd.map(lambda x: (x[0], x[1], int(x[2]*100), x[3]))\n",
        "\n",
        "# confirm transformation is correct\n",
        "rdd_transformation.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoUOjOVtBncy",
        "outputId": "283c2de1-6af0-42bf-db41-4ca7afb244f8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Chris', 1523, 72, 'CA'),\n",
              " ('Jake', 1555, 83, 'NY'),\n",
              " ('Cody', 1439, 92, 'CA'),\n",
              " ('Lisa', 1442, 81, 'FL'),\n",
              " ('Daniel', 1600, 88, 'TX'),\n",
              " ('Kelvin', 1382, 99, 'FL'),\n",
              " ('Nancy', 1442, 74, 'TX'),\n",
              " ('Pavel', 1599, 82, 'NY'),\n",
              " ('Josh', 1482, 78, 'CA'),\n",
              " ('Cynthia', 1582, 94, 'CA')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum_gpa = rdd_transformation.map(lambda x: x[2]).reduce(lambda x,y: x+y)\n",
        "\n",
        "# view the sum\n",
        "sum_gpa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aO91xllWCLAJ",
        "outputId": "51e2767f-80a2-49d2-a432-f8a5f88da4ba"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "843"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum_gpa / rdd_transformation.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GMhpw_DCOjB",
        "outputId": "729fac88-30f7-45ff-de9f-7027a8eff77b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84.3"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Associative and Commutative Properties**"
      ],
      "metadata": {
        "id": "Y4nSIj9FC1mO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "xCq0wE99C7Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [1,2,3,4,5]\n",
        "for i in range(1,5):\n",
        "    rdd = spark.sparkContext.parallelize(data, i)\n",
        "    print('partition: ', rdd.glom().collect())\n",
        "    print('addition: ', rdd.reduce(lambda a,b: a+b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3REu9LigDBgS",
        "outputId": "bf55f383-7bf8-4fe3-c1dc-1932f7fd345d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "partition:  [[1, 2, 3, 4, 5]]\n",
            "addition:  15\n",
            "partition:  [[1, 2], [3, 4, 5]]\n",
            "addition:  15\n",
            "partition:  [[1], [2, 3], [4, 5]]\n",
            "addition:  15\n",
            "partition:  [[1], [2], [3], [4, 5]]\n",
            "addition:  15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,5):\n",
        "    rdd = spark.sparkContext.parallelize(data, i)\n",
        "    print('partition: ', rdd.glom().collect())\n",
        "    print('division: ', rdd.reduce(lambda a,b: a/b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqCIy8jdDIBQ",
        "outputId": "fe687010-544b-4684-e704-5400ae55c799"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "partition:  [[1, 2, 3, 4, 5]]\n",
            "division:  0.008333333333333333\n",
            "partition:  [[1, 2], [3, 4, 5]]\n",
            "division:  3.3333333333333335\n",
            "partition:  [[1], [2, 3], [4, 5]]\n",
            "division:  1.875\n",
            "partition:  [[1], [2], [3], [4, 5]]\n",
            "division:  0.20833333333333331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accumulator & Broadcast Variables**"
      ],
      "metadata": {
        "id": "TCRHUNOFD9hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.getOrCreate()\n",
        "student_data = [(\"Chris\",1523,0.72,\"CA\"),\n",
        "                (\"Jake\", 1555,0.83,\"NY\"),\n",
        "                (\"Cody\", 1439,0.92,\"CA\"),\n",
        "                (\"Lisa\",1442,0.81,\"FL\"),\n",
        "                (\"Daniel\",1600,0.88,\"TX\"),\n",
        "                (\"Kelvin\",1382,0.99,\"FL\"),\n",
        "                (\"Nancy\",1442,0.74,\"TX\"),\n",
        "                (\"Pavel\",1599,0.82,\"NY\"),\n",
        "                (\"Josh\",1482,0.78,\"CA\"),\n",
        "                (\"Cynthia\",1582,0.94,\"CA\")]\n",
        "student_rdd = spark.sparkContext.parallelize(student_data)\n",
        "rdd_transformation = student_rdd.map(lambda x: (x[0], x[1], int(x[2]*100), x[3]))\n",
        "states = {\"NY\":\"New York\", \"CA\":\"California\", \"TX\":\"Texas\", \"FL\":\"Florida\"}\n",
        "broadcastStates = spark.sparkContext.broadcast(states)\n",
        "rdd_broadcast = rdd_transformation.map(lambda x: (x[0],x[1],x[2],broadcastStates.value[x[3]]))"
      ],
      "metadata": {
        "id": "yA2vYliUE1O5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sat_1500 = spark.sparkContext.accumulator(0)\n",
        "\n",
        "# confirm type\n",
        "type(sat_1500)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "3sCMOPfGE7qd",
        "outputId": "2bae41c2-07d3-4528-e4a7-1f736e331aa1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.accumulators.Accumulator"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.accumulators.Accumulator</b><br/>def __init__(aid: int, value: T, accum_param: &#x27;AccumulatorParam[T]&#x27;)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pyspark/accumulators.py</a>A shared variable that can be accumulated, i.e., has a commutative and associative &quot;add&quot;\n",
              "operation. Worker tasks on a Spark cluster can add values to an Accumulator with the `+=`\n",
              "operator, but only the driver program is allowed to access its value, using `value`.\n",
              "Updates from the workers get propagated automatically to the driver program.\n",
              "\n",
              "While :class:`SparkContext` supports accumulators for primitive data types like :class:`int` and\n",
              ":class:`float`, users can also define accumulators for custom types by providing a custom\n",
              ":py:class:`AccumulatorParam` object. Refer to its doctest for an example.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; a = sc.accumulator(1)\n",
              "&gt;&gt;&gt; a.value\n",
              "1\n",
              "&gt;&gt;&gt; a.value = 2\n",
              "&gt;&gt;&gt; a.value\n",
              "2\n",
              "&gt;&gt;&gt; a += 5\n",
              "&gt;&gt;&gt; a.value\n",
              "7\n",
              "&gt;&gt;&gt; sc.accumulator(1.0).value\n",
              "1.0\n",
              "&gt;&gt;&gt; sc.accumulator(1j).value\n",
              "1j\n",
              "&gt;&gt;&gt; rdd = sc.parallelize([1,2,3])\n",
              "&gt;&gt;&gt; def f(x):\n",
              "...     global a\n",
              "...     a += x\n",
              "...\n",
              "&gt;&gt;&gt; rdd.foreach(f)\n",
              "&gt;&gt;&gt; a.value\n",
              "13\n",
              "&gt;&gt;&gt; b = sc.accumulator(0)\n",
              "&gt;&gt;&gt; def g(x):\n",
              "...     b.add(x)\n",
              "...\n",
              "&gt;&gt;&gt; rdd.foreach(g)\n",
              "&gt;&gt;&gt; b.value\n",
              "6\n",
              "\n",
              "&gt;&gt;&gt; rdd.map(lambda x: a.value).collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
              "Traceback (most recent call last):\n",
              "    ...\n",
              "Py4JJavaError: ...\n",
              "\n",
              "&gt;&gt;&gt; def h(x):\n",
              "...     global a\n",
              "...     a.value = 7\n",
              "...\n",
              "&gt;&gt;&gt; rdd.foreach(h) # doctest: +IGNORE_EXCEPTION_DETAIL\n",
              "Traceback (most recent call last):\n",
              "    ...\n",
              "Py4JJavaError: ...\n",
              "\n",
              "&gt;&gt;&gt; sc.accumulator([1.0, 2.0, 3.0]) # doctest: +IGNORE_EXCEPTION_DETAIL\n",
              "Traceback (most recent call last):\n",
              "    ...\n",
              "TypeError: ...</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 60);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_high_sat_score(r):\n",
        "    if r[1] > 1500: sat_1500.add(1)\n",
        "\n",
        "# confirm saved as a function\n",
        "print(count_high_sat_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRVPXFuqE8n4",
        "outputId": "ccaba59b-4c99-480a-c4a1-66667520ac57"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function count_high_sat_score at 0x7912c2442320>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_broadcast.foreach(lambda x: count_high_sat_score(x))\n",
        "\n",
        "# confirm accumulator worked\n",
        "print(sat_1500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApDkdLScFAL9",
        "outputId": "8a4629d0-c83f-4e94-faf0-58e357bd63c7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "mik-XWtgAC0z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}