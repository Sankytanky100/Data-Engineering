# Data Engineering Portfolio

A curated set of hands-on data engineering notebooks demonstrating PySpark analytics and PostgreSQL data modeling workflows. Each notebook is designed to be runnable in Google Colab and includes reproducible steps, context, and outputs suitable for sharing in a technical portfolio.

## Notebooks

### 1) Analyzing Wikipedia Clickstreams with PySpark
**File:** `Analyzing_Wikipedia_Clickstreams_with_PySpark_in_Google_Colab.ipynb`

**What it covers**
- Loads Wikimedia clickstream data into PySpark.
- Cleans and filters large-scale event data for exploratory analysis.
- Performs aggregations to highlight navigation patterns and trends.

**Why it matters**
- Demonstrates distributed data processing with Spark.
- Shows how to translate real-world behavioral datasets into actionable insights.

### 2) Healthcare Data Engineering with PostgreSQL
**File:** `Healthcare_Data_Engineering_with_PostgreSQL_in_Google_Colab.ipynb`

**What it covers**
- Builds relational tables and schemas for a healthcare-style dataset.
- Executes SQL transformations and analytics using PostgreSQL.
- Validates data quality with constraints and query checks.

**Why it matters**
- Highlights schema design, normalization, and SQL-based analytics.
- Demonstrates the ability to model and query structured data.

### 3) PySpark in Colab Tutorial 1
**File:** `Pyspark_in_Colab_Tutorial_1.ipynb`

**What it covers**
- Sets up a Spark environment in Colab.
- Introduces core PySpark DataFrame operations.
- Demonstrates common transformations and actions.

**Why it matters**
- Serves as a foundation for Spark-based projects.
- Illustrates clean, modular data transformation patterns.

### 4) PySpark in Colab Tutorial 2
**File:** `Pyspark_in_Colab_Tutorial_2.ipynb`

**What it covers**
- Expands on advanced DataFrame operations.
- Introduces joins, window functions, and optimization tips.
- Builds intuition for scalable analytical workflows.

**Why it matters**
- Shows readiness for real-world, large-scale Spark jobs.
- Emphasizes performance-aware data engineering.

## How to Run

1. **Open a notebook in Colab**
   - Visit [Google Colab](https://colab.research.google.com/) and upload the notebook you want to run.
2. **Follow the notebook steps**
   - Each notebook includes installation or setup cells where needed (e.g., PySpark or PostgreSQL).
3. **Run all cells top-to-bottom**
   - Colab provides a clean environment; re-run the setup cells if you restart the runtime.

## Recommended Environment

- **Google Colab** (preferred for quick setup)
- **Python 3.9+**
- **PySpark** for Spark notebooks
- **PostgreSQL** for SQL-based workflows

## Portfolio Highlights

- End-to-end data workflows (ingestion → cleaning → transformation → analysis).
- Clear, reproducible steps with emphasis on practical output.
- Organized notebooks ready for interview discussions or live demos.

## Notes for Reviewers

If you are reviewing this repository for hiring or collaboration purposes, each notebook contains a narrative structure and code that can be executed in a fresh Colab environment. The topics were chosen to demonstrate breadth across distributed analytics and relational data modeling.
